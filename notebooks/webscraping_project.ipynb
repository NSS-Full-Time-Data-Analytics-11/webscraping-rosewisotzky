{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b57623",
   "metadata": {},
   "source": [
    "In this exercise, you'll practice using BeautifulSoup to parse the content of a web page. The page that you'll be scraping, https://realpython.github.io/fake-jobs/, contains job listings. Your job is to extract the data on each job and convert into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e4434",
   "metadata": {},
   "source": [
    "1. Start by performing a GET request on the url above and convert the response into a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs/'\n",
    "\n",
    "response = requests.get(URL)\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32611f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d9f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35734a5",
   "metadata": {},
   "source": [
    "a. Use the .find method to find the tag containing the first job title (\"Senior Python Developer\"). Hint: can you find a tag type and/or a class that could be helpful for extracting this information? Extract the text from this title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f397264",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be58a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h2').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(attrs={'class':'title is-5'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19030a",
   "metadata": {},
   "source": [
    "b. Now, use what you did for the first title, but extract the job title for all jobs on this page. Store the results in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd37e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = soup.findAll(attrs={'class':'title is-5'})\n",
    "print(type(job_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5326f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_job = job_titles[0]\n",
    "print(first_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b23c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_titles_text = [x.text for x in job_titles]\n",
    "print(job_titles_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da965d",
   "metadata": {},
   "source": [
    "c. Finally, extract the companies, locations, and posting dates for each job. For example, the first job has a company of \"Payne, Roberts and Davis\", a location of \"Stewartbury, AA\", and a posting date of \"2021-04-08\". Ensure that the text that you extract is clean, meaning no extra spaces or other characters at the beginning or end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa518d",
   "metadata": {},
   "source": [
    "_let's do one at a time here. i think for my approach i can inspect the page and see if there's a class around the div i can use to grab the companies, locations, posting dates. it looks like div .card-content could be useful here. i'm going to make a list of those divs first._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d108da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cards = soup.findAll('div', attrs = {'class':'card-content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80361732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_cards[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078b10b",
   "metadata": {},
   "source": [
    "_ok pivoting back to doing it simply but keeping this beginning here in case i want to come back to it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27476b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_companies = soup.findAll('h3', attrs = {'class':'subtitle is-6 company'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_companies_text = [job.text for job in job_companies]\n",
    "job_companies_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88509e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_locations = soup.findAll('p', attrs = {'class':'location'})\n",
    "job_locations_text = [job.text.strip() for job in job_locations]\n",
    "job_locations_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c041035",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dates = soup.findAll('time')\n",
    "job_dates_text = [job.text.strip() for job in job_dates]\n",
    "job_dates_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada716f",
   "metadata": {},
   "source": [
    "d. Take the lists that you have created and combine them into a pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df = pd.DataFrame({'job_title': job_titles_text,\n",
    "                             'job_company': job_companies_text,\n",
    "                             'job_location': job_locations_text,\n",
    "                              'job_posting_date':job_dates_text})\n",
    "fake_jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00dc13",
   "metadata": {},
   "source": [
    "2. Next, add a column that contains the url for the \"Apply\" button. Try this in two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426a8f3",
   "metadata": {},
   "source": [
    "a. First, use the BeautifulSoup find_all method to extract the urls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757d7e3",
   "metadata": {},
   "source": [
    "_i want to isolate the URLs from the apply button. the problem i'm running into is that there are two a tags w/o  IDs (they have the same class). maybe i can do a for loop to make a list of only the a tags with apply as the text or use a filter in my findall??? the pattern is that we want every other item, perhaps we can separate out odd index number items._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec82488",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = soup.findAll('a')\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_apply_list = []\n",
    "\n",
    "for url in urls:\n",
    "    if url.text == 'Apply':\n",
    "       url_apply_list.append(url)\n",
    "        \n",
    "url_apply_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1820f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_text = [url.get('href') for url in url_apply_list]\n",
    "url_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df['application_link'] = url_text\n",
    "fake_jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enum_urls = list(enumerate(urls))\n",
    "#this was me trying to go odd/even eventually i wanted to loop it and see if it was divisble by two to determine if it was even or odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enum_urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf7957",
   "metadata": {},
   "source": [
    "b. Next, get those same urls in a different way. Examine the urls and see if you can spot the pattern of how they are constructed. Then, build the url using the elements you have already extracted. Ensure that the urls that you created match those that you extracted using BeautifulSoup. Warning: You will need to do some string cleaning and prep in constructing the urls this way. For example, look carefully at the urls for the \"Software Engineer (Python)\" job and the \"Scientist, research (maths)\" job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c52d28",
   "metadata": {},
   "source": [
    "_okay so it looks like the URL pattern is this. each URL starts with https://realpython.github.io/fake-jobs/jobs/ and is then followed by the job title in lowercase separated by dashes, any special characters seem to be replaced with just a space. so museum/gallery is museum-gallery. so i think the first thing i want to do is loop through job titles column and change those to lowercase, replace spaces with dashes and concat with the base URL??????? MAYBE?????_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df['string_manipulation_urls'] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://realpython.github.io/fake-jobs/jobs/'\n",
    "\n",
    "for index, row in fake_jobs_df.iterrows():\n",
    "    print ('index: ', index)\n",
    "    print('row: ', row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566973",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in fake_jobs_df.iterrows():\n",
    "        url_value = row.job_title.lower().replace('(', ' ').replace(')', ' ').replace('/', ' ').replace(',', ' ').replace('  ', ' ').strip().replace(' ', '-')\n",
    "        fake_jobs_df['string_manipulation_urls'] = base_url + url_value + '.html'\n",
    "        \n",
    "fake_jobs_df['string_manipulation_urls']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc192e",
   "metadata": {},
   "source": [
    "3. Finally, we want to get the job description text for each job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4d8fb",
   "metadata": {},
   "source": [
    "a. Start by looking at the page for the first job, https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html. Using BeautifulSoup, extract the job description paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea02662",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_url = 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html'\n",
    "\n",
    "job_response = requests.get(job_url)\n",
    "\n",
    "job_soup = BS(job_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa252d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_soup.find('div', attrs={'class':'content'}).find('p').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0fa2f",
   "metadata": {},
   "source": [
    "b. We want to be able to do this for all pages. Write a function which takes as input a url and returns the description text on that page. For example, if you input \"https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html\" into your function, it should return the string \"At be than always different American address. Former claim chance prevent why measure too. Almost before some military outside baby interview. Face top individual win suddenly. Parent do ten after those scientist. Medical effort assume teacher wall. Significant his himself clearly very. Expert stop area along individual. Three own bank recognize special good along.\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31053353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(page_url):\n",
    "    url = page_url\n",
    "    response = requests.get(url)\n",
    "    soup = BS(response.text)\n",
    "    \n",
    "    description = soup.find('div', attrs={'class':'content'}).find('p').text\n",
    "    \n",
    "    return description\n",
    "\n",
    "scrape('https://realpython.github.io/fake-jobs/jobs/television-floor-manager-8.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a524e63",
   "metadata": {},
   "source": [
    "c. Use the .apply method on the url column you created above to retrieve the description text for all of the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452542d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df['application_link'].apply(scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = fake_jobs_df['application_link'].apply(scrape)\n",
    "type(descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_jobs_df['job_description'] = fake_jobs_df['application_link'].apply(scrape)\n",
    "fake_jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883c952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
